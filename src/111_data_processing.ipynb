{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Techniques and Handling Missing Values\n",
    "\n",
    "In the real world, raw data is rarely perfect. It often contains errors, inconsistencies, and missing values. Data cleaning is the process of detecting and correcting these issues to improve the quality of the data.\n",
    "\n",
    "- *Duplicates*: These are repeated entries in the dataset. Duplicate rows can skew your analysis or model, especially if they represent the same event or entity.\n",
    "- *Incorrect Data Types*: Data that is incorrectly typed (e.g., numbers stored as strings) can lead to errors in analysis and model training.\n",
    "- *Outliers*: Extreme values that are far removed from the rest of the data. They can distort the results of statistical analyses or machine learning models.\n",
    "\n",
    "Missing data can occur for various reasons, such as data entry errors, equipment malfunctions, or privacy concerns. It's essential to handle missing values carefully, as improper handling can lead to biased results.\n",
    "\n",
    "Types of Missing Data:\n",
    "\n",
    "- *MCAR* (Missing Completely at Random): The probability of data being missing is independent of the data itself.\n",
    "- *MAR* (Missing at Random): The probability of missing data is related to other observed data, but not to the missing data itself.\n",
    "- *MNAR* (Missing Not at Random): The missingness is related to the unobserved data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing missing data:**\n",
    "\n",
    "Dropping rows or columns with missing values is the simplest approach, but it can lead to loss of valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../resources/data.csv')\n",
    "print(df)\n",
    "df_cleaned = df.dropna()  # Drops all rows with any missing values\n",
    "# Drop rows with any missing values in numerical columns\n",
    "df_feature_cleaned = df.dropna(subset=['feature1', 'feature2'])\n",
    "print(df_cleaned)\n",
    "print(df_feature_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Imputation**: Replace missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the mean of the respective column\n",
    "df['feature1'].fillna(df['feature1'].mean(), inplace=True)\n",
    "df['feature2'].fillna(df['feature2'].mean(), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median Imputation**: Useful when the data is skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature1'].fillna(df['feature1'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mode Imputation**: Often used for categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature1'].fillna(df['feature1'].mode()[0], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Imputation:\n",
    "\n",
    "**K-Nearest Neighbors (KNN) Imputation**: Estimates missing values based on the values of the nearest neighbors.\n",
    "\n",
    "**Multivariate Imputation**: Considers the relationships between features to impute missing values.\n",
    "\n",
    "**Creating Indicator Variables**: Another approach is to create a binary indicator variable that flags the presence of missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling and Normalization\n",
    "\n",
    "Data scaling and normalization are crucial preprocessing steps that can significantly affect the performance of your models.\n",
    "\n",
    "Data scaling ensures that all features contribute equally to the model's learning process. When features have vastly different ranges, those with larger ranges can dominate the distance calculations or the gradient steps, leading to suboptimal models.\n",
    "\n",
    "*Distance-based Algorithms*: In algorithms like K-Nearest Neighbors (KNN) or Support Vector Machines (SVM), the distance between data points is crucial. If one feature has a much larger range than others, it will disproportionately influence the distance metric, potentially skewing the results.\n",
    "\n",
    "*Gradient Descent*: Algorithms like linear regression and neural networks use gradient descent to minimize a cost function. If the features are not scaled, the gradient descent algorithm may take longer to converge or may converge to a suboptimal solution due to the uneven steps taken in the parameter space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max Scaling\n",
    "\n",
    "Rescales the data to a fixed range, usually [0, 1]. This technique is particularly useful when you know that your data follows a distribution with clear upper and lower bounds.\n",
    "\n",
    "$$\n",
    "X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "$$\n",
    "\n",
    "- Use Min-Max Scaling when you want all your features to have the same scale (e.g., in algorithms like KNN, SVM).\n",
    "- Particularly useful when the data is distributed across a known and fixed range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = {'feature': [10, 50, 100, 150, 200]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../resources/data.csv')\n",
    "\n",
    "# Select numerical columns for scaling\n",
    "numerical_cols = df[['feature1', 'feature2']]\n",
    "\n",
    "# Keep categorical and target columns separate\n",
    "categorical_cols = df[['categorical_column', 'target']]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the numerical columns\n",
    "numerical_scaled = scaler.fit_transform(numerical_cols)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "numerical_scaled_df = pd.DataFrame(numerical_scaled, columns=numerical_cols.columns)\n",
    "\n",
    "# Combine scaled numerical data with the categorical columns\n",
    "df_scaled = pd.concat([numerical_scaled_df, categorical_cols.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z-Score Normalization (or standardization)**\n",
    "\n",
    "Rescales the data to have a mean of 0 and a standard deviation of 1. This technique is useful when the data follows a Gaussian (normal) distribution but not necessarily bounded.\n",
    "\n",
    "$$\n",
    "X' = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "- Use Z-Score Normalization when your data follows a normal distribution.\n",
    "- It's the default choice for many machine learning models, especially those assuming normally distributed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = {'feature': [10, 50, 100, 150, 200]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(df)\n",
    "\n",
    "print(standardized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robust Scaling**\n",
    "\n",
    "Uses the median and interquartile range (IQR) for scaling, which makes it robust to outliers. Unlike Min-Max Scaling and Z-Score Normalization, Robust Scaling is less influenced by extreme values in the data.\n",
    "\n",
    "\n",
    "$$X'=\\frac{X - \\text{median}(X)}{\\text{IQR}(X)}$$\n",
    "\n",
    "Where $\\text{IQR}(X)$ is the range between the 25th percentile ($Q1$) and the 75th percentile ($Q3$).\n",
    "\n",
    "- Use Robust Scaling when your data contains outliers or follows a non-Gaussian distribution.\n",
    "- Ideal for situations where the data is skewed or contains extreme values that should not heavily influence the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = {'feature': [10, 50, 100, 150, 200]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "robust_scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "print(robust_scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Min-Max Scaling*: Use when you know the boundaries of your data or when the algorithm requires data in a specific range.\n",
    "\n",
    "*Z-Score Normalization*: Suitable for normally distributed data or when you want to standardize features to have equal importance.\n",
    "\n",
    "*Robust Scaling*: Best when dealing with data that has outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Feature Engineering\n",
    "\n",
    "Feature engineering is a crucial step in the data preprocessing pipeline. It involves transforming raw data into features that better represent the underlying problem to the predictive models. Good feature engineering can significantly improve the performance of machine learning models by providing them with the most relevant and informative input data.\n",
    "\n",
    "*Feature Engineering* refers to the process of selecting, modifying, or creating new features from raw data to improve the performance of machine learning models. The goal is to create features that capture the underlying patterns in the data more effectively than the original raw data. Proper feature engineering can lead to simpler models, improved accuracy, and reduced training times.\n",
    "\n",
    "**Creating new features** \n",
    "\n",
    "Involves deriving new variables from existing ones that better represent the underlying structure of the data. This could involve mathematical transformations, aggregations, or domain-specific operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'height': [1.60, 1.75, 1.82, 1.90],\n",
    "    'weight': [55, 80, 72, 90]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating a new feature: BMI\n",
    "df['BMI'] = df['weight'] / (df['height'] ** 2)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding Categorical Variables**\n",
    "\n",
    "Machine learning algorithms generally require numerical input. Therefore, categorical variables must be converted into numerical format. There are several techniques to encode categorical variables:\n",
    "\n",
    "*One-hot* encoding converts each category into a new binary column. This method is suitable when there are a limited number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'color': ['red', 'blue', 'green']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['color'])\n",
    "\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Label encoding* assigns a unique integer to each category. It is simple but assumes an ordinal relationship between the categories, which may not be appropriate for all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = {'color': ['red', 'blue', 'green']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "df['color_encoded'] = encoder.fit_transform(df['color'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Target encoding* involves replacing each category with the mean of the target variable for that category. This technique is useful when the categorical variable has a large number of levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'color': ['red', 'blue', 'green', 'red', 'blue', 'green'],\n",
    "    'target': [1, 0, 0, 1, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Target Encoding\n",
    "df['color_encoded'] = df.groupby('color')['target'].transform('mean')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interaction Features**\n",
    "\n",
    "Interaction features are created by combining two or more variables to capture the interactions between them. These features can reveal relationships that are not apparent when considering the variables independently.\n",
    "\n",
    "*Example*:\n",
    "\n",
    "Suppose you have age and income as features. An interaction term could be created by multiplying these two features to capture how income changes with age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'age': [25, 35, 45, 55],\n",
    "    'income': [30000, 50000, 70000, 90000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating an interaction feature\n",
    "df['age_income_interaction'] = df['age'] * df['income']\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial features**\n",
    "\n",
    "Created by generating new features that are polynomial combinations of the existing features. This can be particularly useful when modeling non-linear relationships.\n",
    "\n",
    "*Example*:\n",
    "\n",
    "Suppose you have a single feature x. You can create polynomial features like x^2, x^3, etc., which may help in capturing the non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "data = {'x': [2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(df)\n",
    "\n",
    "# Convert the polynomial features back to a DataFrame\n",
    "df_poly = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['x']))\n",
    "\n",
    "print(df_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection**\n",
    "\n",
    "Identify features that are highly correlated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns for correlation\n",
    "numerical_df = df.select_dtypes(include=[float, int])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numerical_df.corr()\n",
    "\n",
    "# Print the correlations with the 'target' variable, sorted in descending order\n",
    "print(correlation_matrix['target'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Notes on scikit-learn\n",
    "\n",
    "One of the most popular and powerful open-source libraries in Python for machine learning and data analysis. It provides a comprehensive suite of tools for building, evaluating, and deploying machine learning models, making it an essential tool for data scientists, machine learning engineers, and statisticians.\n",
    "\n",
    "*Pipelines* in scikit-learn allow you to streamline the process of creating machine learning workflows by combining multiple steps (e.g., preprocessing, model fitting) into a single, cohesive process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = [[1, 10], [100, 50], [5, 1 ], [10, 100], [50, 5]]\n",
    "y_train = [0, 0, 1, 1, 0 ]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation Metrics**:\n",
    "\n",
    "Scikit-learn includes a rich set of metrics for evaluating model performance, from standard accuracy to more complex metrics like ROC AUC, mean squared error (MSE), and adjusted R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0] \n",
      "\n",
      "0.6 \n",
      "\n",
      "[[3 1]\n",
      " [1 0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "X_test = [[1, 10], [10, 5], [5, 1 ], [10, 100], [50, 5]]\n",
    "y_test = [0, 1, 0, 0, 0 ]\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(y_pred, '\\n')\n",
    "print(accuracy, '\\n')\n",
    "print(cm, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
